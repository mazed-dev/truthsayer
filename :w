// Load wink-nlp package.
import winkNLP, { PartOfSpeech, Bow, WinkMethods } from 'wink-nlp'
// Load english language model.
import model from 'wink-eng-lite-web-model'

const kOkapiBM25PlusB = 0.75
const kOkapiBM25PlusK1 = 2.0
const kOkapiBM25PlusDelta = 1.0

/**
 * Index implementation for Okapi BM25+
 * https://en.wikipedia.org/wiki/Okapi_BM25
 */
export type OkapiBM25PlusIndex = {
  algorithm: 'Okapi BM25+'
  // Keep version around in case we need to patch the implementation and
  // regenrate an index.
  version: 1
  // Number of documents containing term (lemma)
  // NOTE (akindyakov): It might be tempting to use "bow" acronym for "bag of
  // words" here, please don't. Because it's just too confusing in a large
  // enough codebase.
  bagOfwords: Record<string, number>
  documentsNumber: number
  wordsInAllDocuments: number

  model: NlpModel
}

export type OkapiBM25PlusPerDocumentIndex = {
  algorithm: 'Okapi BM25+'
  version: 1
  // Unique nid for a document
  nid: string
  // Number of times a term (lemma) occurs in the document
  // NOTE (akindyakov): It might be tempting to use "bow" acronym for "bag of
  // words" here, please don't. Because it's just too confusing in a large
  // enough codebase.
  bagOfwords: Record<string, number>
  wordsNumber: number
}

export type NlpModel = {
  wink: WinkMethods
}

export function init(): NlpModel {
  // Instantiate winkNLP model
  const wink = winkNLP(model)
  return { wink }
}

export function createIndex(
  model: NlpModel
): [OkapiBM25PlusIndex, OkapiBM25PlusPerDocumentIndex[]] {
  return [
    {
      algorithm: 'Okapi BM25+',
      version: 1,
      bagOfwords: {},
      documentsNumber: 0,
      wordsInAllDocuments: 0,
      model,
    },
    [],
  ]
}

function createPerDocumentIndex({
  bagOfwords,
  wordsNumber,
  nid,
}: {
  bagOfwords: Record<string, number>
  wordsNumber: number
  nid: string
}): OkapiBM25PlusPerDocumentIndex {
  return {
    algorithm: 'Okapi BM25+',
    version: 1,
    bagOfwords,
    wordsNumber,
    nid,
  }
}

function addRecordValue<K extends keyof any>(
  rec: Record<K, number>,
  key: K,
  by: number,
  defaultValue?: number
): Record<K, number> {
  const value = rec[key] ?? defaultValue ?? 0
  rec[key] = value + by
  return rec
}

function createPerDocumentIndexFromText(
  text: string,
  nid: string,
  model: NlpModel,
): OkapiBM25PlusPerDocumentIndex {
  const { wink } = model
  const doc = wink.readDoc(text)
  const tokenTypes = doc.tokens().out(wink.its.type)
  const lemmas = doc
    .tokens()
    .out(wink.its.lemma)
    .filter((_lemma: string, index: number) => {
      // Filter out punctuation
      return tokenTypes[index] !== 'punctuation'
    })
  const bagOfwords = lemmas.reduce((bagOfwords: Bow, lemma: string) => {
    return addRecordValue(bagOfwords, lemma, 1)
  }, {})
  const wordsNumber = lemmas.length
  return createPerDocumentIndex({
    bagOfwords,
    wordsNumber,
    nid,
  })
}

export function addDocument(
  relIndex: OkapiBM25PlusIndex,
  text: string,
  nid: string
): [OkapiBM25PlusIndex, OkapiBM25PlusPerDocumentIndex] {
  const doc = createPerDocumentIndexFromText(text, nid, relIndex.model)
  relIndex.wordsInAllDocuments += doc.wordsNumber
  relIndex.documentsNumber += 1
  for (const word in doc.bagOfwords) {
    relIndex.bagOfwords = addRecordValue(relIndex.bagOfwords, word, 1)
  }
  return [relIndex, doc]
}

function getTermInDocumentImportance(
  occurenceInDoc: number,
  documentSizeInWords: number,
  averageDocumentSizeInWords: number
): number {
  return (
    kOkapiBM25PlusDelta +
    (occurenceInDoc * (kOkapiBM25PlusK1 + 1)) /
      (occurenceInDoc +
        kOkapiBM25PlusK1 *
          (1 -
            kOkapiBM25PlusB +
            (kOkapiBM25PlusB * documentSizeInWords) /
              averageDocumentSizeInWords))
  )
}

function getTermInDocumentScore(
  term: string,
  relIndex: OkapiBM25PlusIndex,
  doc: OkapiBM25PlusPerDocumentIndex,
  averageDocumentSizeInWords: number
) {
  const occurenceInDoc = doc.bagOfwords[term]
  if (occurenceInDoc == null) {
    return 0
  }
  // prettier-ignore
  return (
      getTermInverseDocumentFrequency(term, relIndex) * getTermInDocumentImportance(
        occurenceInDoc,
        doc.wordsNumber,
        averageDocumentSizeInWords,
      )
    )
}

function getKeyphraseInDocumentScore(
  keywords: string[],
  relIndex: OkapiBM25PlusIndex,
  doc: OkapiBM25PlusPerDocumentIndex
) {
  const averageDocumentSizeInWords =
    relIndex.wordsInAllDocuments / relIndex.documentsNumber
  return keywords
    .map((term: string, _index: number) =>
      getTermInDocumentScore(term, relIndex, doc, averageDocumentSizeInWords)
    )
    .reduce((prev: number, current: number) => current + prev)
}

type SearchResultDocument = {
  doc: OkapiBM25PlusPerDocumentIndex
  score: number
}

/**
 * Search for key phrase made by human, a classic implementation of Okapi BM25+
 * See https://en.wikipedia.org/wiki/Okapi_BM25
 *
 * It's slightly simpler than `findRelevantDocuments`, this one doesn't take
 * into account importance score of each term in `keyphrase`, assuming that
 * human know what they do when they type keyphrase.
 */
export function findRelevantDocumentsForPhrase(
  keyphrase: string,
  limit: number,
  relIndex: OkapiBM25PlusIndex,
  docs: OkapiBM25PlusPerDocumentIndex[]
): SearchResultDocument[] {
  const doc = createPerDocumentIndexFromText(keyphrase, '', relIndex.model)
  const lemmas = Object.keys(doc.bagOfwords)
  const results: SearchResultDocument[] = []
  docs.forEach((doc: OkapiBM25PlusPerDocumentIndex) => {
    const score = getKeyphraseInDocumentScore(
      lemmas,
      relIndex,
      doc
    )
    if (score > 1) {
      results.push({ doc, score })
    }
  })
  results.sort((ar, br) => ar.score - br.score)
  return results.slice(-limit)
}

/**
 * Importance of the term in the entire corpus (all documents) calculated as
 * term inverse document frequency.
 */
export function getTermInverseDocumentFrequency(
  term: string,
  relIndex: OkapiBM25PlusIndex
) {
  const numberOfDocumentsContainingTerm = relIndex.bagOfwords[term] ?? 0
  return Math.log(
    1 +
      (relIndex.documentsNumber - numberOfDocumentsContainingTerm + 0.5) /
        (numberOfDocumentsContainingTerm + 0.5)
  )
}

function getTextRelevanceScore(
  queryDoc: OkapiBM25PlusPerDocumentIndex,
  relIndex: OkapiBM25PlusIndex,
  corpusDoc: OkapiBM25PlusPerDocumentIndex
): number {
  const averageDocumentSizeInWords =
    relIndex.wordsInAllDocuments / relIndex.documentsNumber
  const score = Object.entries(queryDoc.bagOfwords)
    .map(([term, occurenceInQueryDoc]) => {
      const occurenceInCorpusDoc = corpusDoc.bagOfwords[term]
      if (occurenceInCorpusDoc == null) {
        return 0
      }
      // prettier-ignore
      return (
      getTermInverseDocumentFrequency(term, relIndex) * getTermInDocumentImportance(
        occurenceInCorpusDoc,
        corpusDoc.wordsNumber,
        averageDocumentSizeInWords,
      ) * getTermInDocumentImportance(
        occurenceInQueryDoc,
        queryDoc.wordsNumber,
        averageDocumentSizeInWords,
      )
    )
    })
    .reduce((prev: number, current: number) => current + prev)
  return score
}

export function findRelevantDocuments(
  text: string,
  limit: number,
  relIndex: OkapiBM25PlusIndex,
  docs: OkapiBM25PlusPerDocumentIndex[]
): SearchResultDocument[] {
  const queryDoc = createPerDocumentIndexFromText(text, '', relIndex.model)
  const results: SearchResultDocument[] = []
  docs.forEach((corpusDoc: OkapiBM25PlusPerDocumentIndex) => {
    const score = getTextRelevanceScore(queryDoc, relIndex, corpusDoc)
    if (score > 1) {
      results.push({ doc: corpusDoc, score })
    }
  })
  results.sort((ar, br) => br.score - ar.score)
  return results.slice(0, limit)
}

const kHotPartsOfSpeach: Set<PartOfSpeech> = new Set([
  'NOUN',
  'ADJ',
  'PROPN',
  'NUM',
  'VERB',
])
type Keyphrase = {
  phrase: string
  score: number
}
export function extractSearchKeyphrases(
  text: string,
  relIndex: OkapiBM25PlusIndex
): Keyphrase[] {
  console.log('extractSearchKeyphrases', text)
  const { wink } = relIndex.model
  const doc = wink.readDoc(text)
  const phrases: Keyphrase[] = []
  doc.sentences().each((s) => {
    const partsOfSpeach = s.tokens().out(wink.its.pos) as PartOfSpeech[]
    const lemmas = s.tokens().out(wink.its.lemma)
    // Calculate keyphrases score as a multiplication of IDFs of all words
    let score = 0
    const phrase = s
      .tokens()
      .out()
      .map((token: string, index: number) => {
        const partOfSpeach = partsOfSpeach[index]
        if (kHotPartsOfSpeach.has(partOfSpeach)) {
          const lemma = lemmas[index]
          const idf = getTermInverseDocumentFrequency(lemma, relIndex)
          console.log(
            token,
            partOfSpeach,
            lemma,
            idf,
            index,
            relIndex.bagOfwords[lemma] ?? 0,
            relIndex.documentsNumber
          )
          // Ignore fequent words
          if (idf > 1.0) {
            score += idf
            return token
          }
        }
        return ''
      })
      .filter((s) => !!s)
      .join(' ')
    phrases.push({
      phrase,
      score,
    })
  })
  console.log('Phrase', phrases)
  return phrases
}
